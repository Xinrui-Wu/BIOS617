---
title: "BIOS 617 - Lecture 4"
author: "Walter Dempsey"
date: "1/22/2019"
output:
  beamer_presentation: default
  ioslides_presentation:
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## JITT #3

- Show $\bar y_{h}$ under SRS from the strata is unbiased 
- Show population estimate $\bar y_{st}$ is unbiased
- What is the difference between stratification sampling and subdomain estimation?

## Unbiasedness

* What is the probability of sampling individual $j$ from strata $h$?
$$\small
\begin{aligned}
P(I_{jh} = 1) &= 1 - P(I_{jh} \neq 1) \\
&= 1 - \frac{N_h-1}{N_h} \times \frac{N_h-2}{N_h-1} \times \cdots \times \frac{N_h-n}{N_h-(n_h-1)} \\
&= 1 - \frac{N_h-n_h}{N_h} = \frac{n_h}{N_h}
\end{aligned}
$$
* Then by linearity of expectations
$$\small 
\begin{aligned}
E [ \bar y_h ] &= E \left[ \frac{1}{n_h} \sum_j I_{jh} Y_{jh} \right] = \frac{1}{n} \sum_j E [ I_{jh}  ] Y_{jh} \\
&= \frac{1}{n_h} \sum_j \frac{n_h}{N_h} Y_{jh} = \bar Y_h
\end{aligned}
$$ 

## Unbiasedness

* Linearity of expectations
$$\small 
\begin{aligned}
E [ \bar y ] &= E \left[ \sum_h P_h \bar y_h \right] \\
&= \sum_h P_h \bar Y_h \\
&= \sum_h \left[ \frac{N_h}{N} \times \left[ \frac{1}{N_h} \sum_j Y_{jh} \right] \right] \\
&= \frac{1}{N} \sum_{j,h} Y_{jh} = \bar Y
\end{aligned}
$$ 

## Subdomain estimation vs stratified sampling

- Subdomain estimation: cannot ascertain whether an element in the population is a member of the subdomain *until* we
sample it.
- Stratified sampling: we know whether an element in the population is a member of the strata *before* we sample it.
- The sample estimator $\bar y_j = n_j^{-1} \sum_{i=1}^n D_{ij} y_i$ with $n_{j} = \sum_{i=1}^n D_{ij}$
is unbiased for the population subdomain $\bar Y_j = N_j^{-1} \sum_{i=1}^N D_{ij} Y_i$ for $N_j = \sum_{i=1}^N D_{ij}$
- The sample estimator $\bar y_h = n_h^{-1} \sum_{i=1}^{n_h} y_{ih}$ is unbiased for strata population mean $\bar Y_h$
- Where's the difference?


## Your second JITT:  3rd proof of variance
- We know $V (\bar y) = V\left( \frac{1}{n} \sum_{i=1}^n I_i Y_i \right)$ 
- Hint 1: $V(I_i) = f (1-f)$ and $\text{Cov}(I_i, I_j) = f \left( \frac{n-1}{N-1} - f \right)$
- Step 1: Use hint 1 to show the variance is equivalent to
$$ \small
\frac{f (1-f)}{n^2} \left[ \sum_{i=1}^N Y_i^2 - \frac{1}{N-1} \sum_{j=1}^N \sum_{i\neq j} Y_i Y_j\right]
$$
- Hint 2:
$$ \small
\sum_{j=1}^N \sum_{i=1}^N Y_i Y_j = \left( \sum_{i=1}^N Y_i \right)^2 - \sum_{i=1}^N Y_i^2
$$
- Step 2: Combine hint and variance formula to show result

## JITT #2: Calculation

$$\small 
\begin{aligned}
V(\bar y) &= V\left( \frac{1}{n} \sum_{i=1}^N I_j Y_j \right) = \frac{1}{n^2} \left[ \sum_{i=1}^N V(I_j) Y_j^2 + \sum_{i \neq j} \text{Cov} (I_i, I_j) Y_i Y_j \right] \\
&= \frac{1}{n^2} \left[ f(1-f) \sum_{i=1}^N Y_j^2 + f \left ( \frac{n-1}{N-1} - f \right) \sum_{i \neq j} Y_i Y_j \right] \\
&= \frac{f(1-f)}{n^2} \left[ \sum_{i=1}^N Y_j^2 + \frac{\left ( \frac{n-1}{N-1} - f \right)}{1-f} \sum_{i \neq j} Y_i Y_j \right] \\
&= \frac{f(1-f)}{n^2} \left[ \sum_{i=1}^N Y_j^2 -\frac{1}{N-1} \sum_{i \neq j} Y_i Y_j \right] \\
\end{aligned}
$$ 

## JITT #2: Calculation (ctd.)
$$\small 
\begin{aligned}
V(\bar y) &= \frac{f(1-f)}{n^2} \left[ \sum_{i=1}^N Y_j^2 -\frac{1}{N-1} \sum_{i \neq j} Y_i Y_j \right] \\
&= \frac{f(1-f)}{n^2} \left[ \sum_{i=1}^N Y_j^2 -\frac{1}{N-1} \left( \left( \sum_{i=1}^N Y_i \right)^2 - \sum_{i=1}^N Y_i^2 \right) \right] \\
&= \frac{f(1-f)}{n^2} \left[ \frac{N}{N-1} \sum_{i=1}^N Y_j^2 -\frac{1}{N-1} \left( \sum_{i=1}^N Y_i \right)^2 \right] \\
&= \frac{f(1-f)}{n^2} \frac{N}{N-1} \left[ \sum_{i=1}^N Y_j^2 -\frac{1}{N} \left( \sum_{i=1}^N Y_i \right)^2 \right] \\
&= \frac{f(1-f)}{n^2} \frac{N}{N-1} \left[ \sum_{i=1}^N Y_j^2 - N \bar Y^2 \right] \\
&= \frac{f(1-f)}{n^2} N S^2 = (1-f)\frac{S^2}{n}\\
\end{aligned}
$$ 

## Optimal allocation (i.e., today's topic)

* While proportionate allocation usually reduces variances over simple random sampling if there are differences in means, it is not
the most efficient method possible if there are also differing variances by stratum
* **Goal**: Minimize the variance subject to cost constraints
* Mathematically,
$$
(n_1,\ldots,n_H) = \arg \min V(\bar y_{st}) = \arg \min \sum_h P_h (1-f_h) \frac{S_h^2}{n_h}
$$
* Subject to a fixed total cost $C$. 
* We will assume a general setting where the average cost per interview is equal to $c_h$ and thus, subtracting out any fixed
costs, the budget available is $C = \sum_h c_h n_h$.

## Optimal allocation

Since
$$ \small
\begin{aligned}
V(\bar y_{st}) &= \sum_h P_h (1-f_h)  \frac{S_h^2}{n_h} \\
&= \sum_h P_h^2 \frac{S_h^2}{n_h} - \sum_h \frac{N_h^2}{N^2} \frac{n_h}{N_h} \frac{S_h^2}{n_h} \\
&= \sum_h P_h^2 \frac{S_h^2}{n_h} - N^{-1} \sum_h P_h S_h^2 \\
\end{aligned}
$$
* Second term does not include $n_h$ so we can simply optimize the first term (+ a Langrange multiplier):
$$
\phi(n_1, \ldots, n_H) = \sum_h P_h^2 S_h^2/n_h + \lambda \left( \underbrace{\sum_h c_h n_h - C}_{=0} \right)
$$

## Optimal allocation

* Differentiate with respect to $n_h$:
$$
\frac{\partial \phi(n_1, \ldots, n_H)}{\partial n_h} = - \frac{P_h^2 S_h^2}{n_h^2} + \lambda c_h = 0
$$
* So $n_h \propto \frac{P_h S_h}{\sqrt{\lambda \cdot c_h}}$. Next, differentiate with respect $\lambda$ yields
$$
C = \sum_h c_h n_h = \sum_h c_h \frac{P_h S_h}{\sqrt{\lambda \cdot c_h}} \Rightarrow \sqrt{\lambda} = \frac{\sum_h P_h S_h^2 \sqrt{c_h}}{C}  
$$
* This leads to 
$$
n_h = \frac{C}{\sum_{\tilde h} \sqrt{c_{\tilde h}} P_{\tilde h} S_{\tilde h} } \times \frac{P_h S_h}{\sqrt{c_h}}
$$
* This is consistent with intuition: (a) larger samples in strata with larger variances, and (b) smaller samples in strata with higher costs.


## Neyman allocation

* In the (common) setting where per-stratum costs are constant ($c_h = c$ for all $h=1,\ldots,H$ so $C = n \cdot c$), the optimal allocation results simplify and we have
$$
n_h = \frac{C}{\sum_{\tilde h} \sqrt{c_{\tilde h}} P_{\tilde h} S_{\tilde h} } \times \frac{P_h S_h}{\sqrt{c_h}} = n \frac{P_h S_h}{\sum_{\tilde h} P_{\tilde h} S_{\tilde h}}
$$
so $n_h \propto P_h S_h$
* Termed Neyman allocation because these optimation issues were first considered by Jerzey Neyman in his work developing the fundamentals of probability sampling 
* Less familiar, but arguably as important, as his work in the foundations of statistical inference

## Variance of neyman allocation

$$ \small
\begin{aligned}
V(\bar y_N) &= \sum_h P_h^2 (1-f_h) S_h^2 / n_h \\
&= \sum_h P_h^2 S_h^2 / n_h - N^{-1} \sum_h P_h S_h^2\\ 
&= \sum_h \frac{P_h^2 S_h^2}{n \frac{P_h S_h}{\sum_{\tilde h} P_{\tilde h} S_{\tilde h}}} - N^{-1} \sum_h P_h S_h^2 \\ 
&= \frac{\left( \sum_h P_h S_h \right)^2}{n} - N^{-1} \sum_h P_h S_h^2\\
&\approx \frac{\left( \sum_h P_h S_h \right)^2}{n}\\ 
\end{aligned}
$$
where the last line holds when $N$ is big and $N \gg n$.

## Optimal allocation

* When $N_h$ is small, optimal allocation may suggest $n_h > N_H$
* Set $f_h = 1$ and re-run the optimal allocation with
$$
\tilde C = C- \sum_h I(f_h=1) N_h c_h
$$
in the case of optimal allocation __or__
$$
\tilde n = n - \sum_h I(f_h = 1) N_h
$$
in the case of Neyman allocation
* We drop those strata in which a census is taken. Why can we drop those strata?

## The gains from Neyman Allocation

* What do we gain by considering Neyman allocation in design?
* 
$$
\begin{aligned}
V(\bar y_{prop}) - V(\bar y_N ) 
&= \left[ \frac{\sum_h P_h S_h^2}{n}- N^{-1} \sum_h P_h S_h^2 \right] \\
&- \left[ \frac{\left( \sum_h P_h S_h \right)^2}{n} - N^{-1} \sum_h P_h S_h^2 \right] \\
&= n^{-1} \left( \sum_h P_h S_h^2 - \left( \sum_h P_h S_h \right)^2 \right) \\
&= n^{-1} \sum_h P_h (S_h - \bar S)^2
\end{aligned}
$$
So
$$
\frac{V(\bar y_{prop}) - V(\bar y_N )}{V(\bar y_N )} \approx \frac{\sum_h P_h (S_h - \bar S)^2}{\sum_h P_h S_h^2}
$$

```{r gamma, echo = FALSE, include = FALSE}
# install packages
if(!require('dslabs')){install.packages('dslabs')}
if(!require('tidyverse')){install.packages('tidyverse', dependencies = TRUE)}
if(!require('ggrepel')){install.packages('ggrepel')}
if(!require('matrixStats')){install.packages('matrixStats')}

# load libraries
library(dslabs)
library(tidyverse)
library(ggrepel)
library(matrixStats)

set.seed(145616516)
S.sq = rgamma(n = 10, 10,2)
df = data.frame(S.sq)
denom = sum(S.sq^2)
numer = sum((S.sq - mean(S.sq))^2)
```

## A toy example
- Generate 10 strata each with stratum variance from $\Gamma(10,2)$
- $S^2 = (`r round(S.sq,3)` )$ 
- $P_h = 1/`r length(S.sq)`$
- Then 
$$
\frac{\sum_h P_h (S_h - \bar S)^2}{\sum_h P_h S_h^2} = \frac{`r round(numer,3)`}{`r round(denom,3)`} = `r round(numer/denom,3)`
$$

## A toy example

```{r pew, echo = FALSE}
df %>%  ggplot(aes(S.sq)) +
  geom_histogram(bins = 7, fill = "blue") +
  xlab("Stratum variance") +
  ylab("Frequency") +
  ggtitle("Histogram of stratum variance")
```

## Optimal allocation 
- Small departures from optimal allocation have modest impact on variance
- Neyman allocation: $n_h^\prime = n P_h S_h / \sum_{\tilde h} P_{\tilde h} S_{\tilde h}$
- So $P_h S_h = n_h^\prime \sum_{\tilde h} P_{\tilde h} S_{\tilde h} / n = n^\prime_h \bar S / n$
- Then for general allocation, we have
$$
\begin{aligned}
V(\bar y_{st}) - V(\bar y_N) &= \sum_h P_h^2 S_h^2/n_h - \bar S^2/n \\
&= \sum_h \left( \frac{n_h^\prime \bar S}{n \cdot n_h} \right)^2 - \bar S^2/n \\
&= \frac{\bar S^2}{n^2} \sum_h \left[ \frac{(n_h^\prime)^2}{n_h} - n \right] \\
\end{aligned}
$$

## Optimal allocation 
* Using the identity
$$
\sum_h \frac{(n_h^\prime-n_h)^2}{n_h} = \sum_h \frac{(n_h^\prime)^2}{n_h} - 2 \sum_h n_h^\prime + \sum_h n_h = \sum_h \frac{(n_h^\prime)^2}{n_h} - n
$$
* Ignoring the finite population correction, we have
$$
\frac{V(\bar y_{st}) - V(\bar y_N)}{V(\bar y_N)} \approx \frac{1}{n} \sum_h \frac{(n_h^\prime-n_h)^2}{n_h} = \frac{1}{n} \sum_h n_h g_h^2 = \sum_h p_h g_h^2
$$
for $g_h = (n_h^\prime - n_h)/n_h$

* Proportionate increase in variance = weighted average of the squared change in the percentage change in allocation from the optimum.
  + A 10% deviation in the allocation from the optimum yields a 1% increase in the variance for any one stratum.

## afjal

* Note that optimal allocation is statistic specific: it is with respect to the mean of a given variable.
  + The resulting allocation might not be optimal for a different variable, or even a different statistic associated with the
same variable (e.g., a percentile rather than a mean). 
* One also has to be able to estimate $S_h^2$ and $c_h$ to use optimal allocation.
  + Similar to power calculation – one must have some source of external information about variability. (Binary, binomial,
Possion (count) data – variances are a function of the means, which are often easier to estimate/speculate about.)

## Stratified Sampling and Proportions

## JITT #4

- Do your HW
